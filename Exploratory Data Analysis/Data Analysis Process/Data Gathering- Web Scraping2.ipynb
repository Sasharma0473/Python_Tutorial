{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cee0160",
   "metadata": {},
   "source": [
    "# BAISC SCRAPING => SCRAPE DATA FROM AMBITION BOX\n",
    "Documentation \n",
    "\n",
    "https://selenium-python.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d455f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we get Response as 200 means everything is fine\n",
    "# If we get response as 403 means its a bad request and server has not responded it has rejected our request.\n",
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometime what happens we can'nt scrap the data directly. when we hit this url then it feel like for the website that this \n",
    "# request is coming from some bot which is going to extract the data so what we do is we make the website feel like that \n",
    "# the request itself is genearted from the brower.\n",
    "requests.get('https://www.ambitionbox.com/list-of-companies?page=1').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are able to get the webpage now we have to parse it to get the right information with the help of Beautiful Soup\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "webpage = requests.get('https://www.ambitionbox.com/list-of-companies?page=1', headers = headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lxml- Its a html parser which make the traversing easy in webpage. We will give this webpage to beautiful soup. \n",
    "# and create one object which will help us to use the method in beautiful soup.\n",
    "soup = BeautifulSoup(webpage,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2f14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all('h1')[0].text\n",
    "soup.find_all('h1')[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0427ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(soup.find_all('h2'))\n",
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('h2'):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71798f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('span'):\n",
    "    print(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span',class_ ='companyCardWrapper__companyRatingValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span',class_='companyCardWrapper__ActionTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a',class_='companyCardWrapper__ActionWrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data Through DIV container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = soup.find_all('div',class_='companyCardWrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company[0].find_all('span',class_='companyCardWrapper__ActionCount')[1].text.strip()\n",
    "name=[]\n",
    "rating = []\n",
    "review_count=[]\n",
    "salary=[]\n",
    "interview=[]\n",
    "job=[]\n",
    "benifit=[]\n",
    "photo=[]\n",
    "\n",
    "for i in company:\n",
    "    name.append(i.find('h2').text.strip())\n",
    "    rating.append(i.find('span',class_='companyCardWrapper__companyRatingValue').text.strip())\n",
    "    review_count.append(i.find('span',class_='companyCardWrapper__ActionCount').text.strip())\n",
    "    salary.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[1].text.strip())\n",
    "    interview.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[2].text.strip())\n",
    "    job.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[3].text.strip())\n",
    "    benifit.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[4].text.strip())\n",
    "    photo.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[5].text.strip())\n",
    "# print(name)\n",
    "# print(rating)\n",
    "# print(review_count)\n",
    "# print(salary)\n",
    "# print(interview)\n",
    "# print(benifit)\n",
    "# print(photo)\n",
    "\n",
    "df= {\"name\":name,\"Rating\":rating,\"Review_count\":review_count,\"Salary\":salary,\"Interview\":interview,\"Job\":job,\"Benifts\":benifit,\"Photo\":photo}\n",
    "company_data = pd.DataFrame(df)\n",
    "company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for j in range(1,10):\n",
    "    url = 'https://www.ambitionbox.com/list-of-companies?page={}'.format(j)\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "    webpage = requests.get(url, headers = headers).text\n",
    "    soup = BeautifulSoup(webpage,'lxml')\n",
    "    company = soup.find_all('div',class_='companyCardWrapper')\n",
    "    # company[0].find_all('span',class_='companyCardWrapper__ActionCount')[1].text.strip()\n",
    "    \n",
    "    name=[]\n",
    "    rating = []\n",
    "    review_count=[]\n",
    "    salary=[]\n",
    "    interview=[]\n",
    "    job=[]\n",
    "    benifit=[]\n",
    "    photo=[]\n",
    "    \n",
    "    for i in company:\n",
    "        name.append(i.find('h2').text.strip())\n",
    "        rating.append(i.find('span',class_='companyCardWrapper__companyRatingValue').text.strip())\n",
    "        review_count.append(i.find('span',class_='companyCardWrapper__ActionCount').text.strip())\n",
    "        salary.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[1].text.strip())\n",
    "        interview.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[2].text.strip())\n",
    "        job.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[3].text.strip())\n",
    "        benifit.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[4].text.strip())\n",
    "        photo.append(i.find_all('span',class_='companyCardWrapper__ActionCount')[5].text.strip())\n",
    "        \n",
    "    # print(name)\n",
    "    # print(rating)\n",
    "    # print(review_count)\n",
    "    # print(salary)\n",
    "    # print(interview)\n",
    "    # print(benifit)\n",
    "    # print(photo)\n",
    "\n",
    "    df= {\"name\":name,\"Rating\":rating,\"Review_count\":review_count,\"Salary\":salary,\"Interview\":interview,\"Job\":job,\"Benifts\":benifit,\"Photo\":photo}\n",
    "    company_data = pd.DataFrame(df)\n",
    "    print(pd.concat(final,company_data))\n",
    "    # final.append(company_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca6d62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_data =np.vstack((final[0],final[1],final[2],final[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ee05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(tot_data)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2dc0b",
   "metadata": {},
   "source": [
    "# ADVANCE WEB SCRAPING FROM SMARTPRIX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3f348",
   "metadata": {},
   "source": [
    " We can scrape the data from request module but it has its limitation. The limitation is that it will fecth the data \n",
    "where each and every page number is defined. data is defined by page number just like i ambition box website\n",
    "\n",
    "But if the webpage is dynamic and it is loading data or html code on its own or we have to click LOAD button in the \n",
    "website to load the data then we will use selenium to scarpe data from dynamic website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5fea88",
   "metadata": {},
   "source": [
    "# ABOUT SELENIUM\n",
    "\n",
    "Selenium is tool for automating web brower. Its primarily used for testing web application and it can be used for web scaping, data extarction and other task that require interaction with the web brower programatically.\n",
    "\n",
    "with selenium you can write code in variety of programming language including python, java, C#, to control the web brower \n",
    "and interact with the pages. you can use selenium to simulate user inetraction such as clicking button , filling out forms \n",
    "and navigating between the pages to test how a web application behaves.\n",
    "\n",
    "Its also used for web scraping, where you can extract information from website and save it for further analysis and use .\n",
    "Its a best choice for web scraper to interact with the with the dynamic website, allowing you to scrape information from \n",
    "pages that are generated by javascript. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc4fa5",
   "metadata": {},
   "source": [
    "# SET UP \n",
    "Install selenium -- pip install selenium \n",
    "\n",
    "Download chrome driver-- selenium can work with any driver. like andorid , chorme etc It has different driver to access information from different platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6487a10",
   "metadata": {},
   "source": [
    "# Task \n",
    "With the help of selenium we will perform these task automatically without doing manually. \n",
    "\n",
    "1). open google.com\n",
    "\n",
    "2). Search Campusx \n",
    "\n",
    "3). learnwith.campusx.in\n",
    "\n",
    "4). dsmp course page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver # This will hep in opening the web brower\n",
    "from selenium.webdriver.chrome.service import Service # it will help in running the driver \n",
    "from selenium.webdriver.common.by import By  \n",
    "from selenium.webdriver.common.keys import Keys    # we can import this for enter key, shift key ,alt key from keyborad\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"C:/Users/sasha/Desktop/chromedriver-win64/chromedriver.exe\")   # we have created service object provide the path of the chrome driver \n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we can navigate to the website with help of automated code.\n",
    "\n",
    "driver.get(\"https://www.google.com/\")\n",
    "time.sleep(2)  # It will delay the search by 2 sec. Then will search any website on it after opening the google.com\n",
    "\n",
    "# Fetch the search input box using xpath\n",
    "\n",
    "user_input = driver.find_element(by=By.XPATH, value='//*[@id=\"APjFqb\"]')\n",
    "user_input.send_keys('Campusx')\n",
    "time.sleep(1)\n",
    "user_input.send_keys(Keys.ENTER)\n",
    "time.sleep(1)\n",
    "link = driver.find_element(by=By.XPATH,value='//*[@id=\"rso\"]/div[1]/div/div/div/div/div/div/div/div[1]/div/span/a')\n",
    "link.click()\n",
    "time.sleep(1)\n",
    "\n",
    "link3 = driver.find_element(by=By.XPATH,value='/html/body/div[1]/header/section[2]/a[1]')\n",
    "link3.click()\n",
    "time.sleep(1)\n",
    "\n",
    "link4 = driver.find_element(by= By.XPATH,value = '/html/body/div[2]/div/div[5]/div[1]/a')\n",
    "link4.click()\n",
    "\n",
    "# Doubt \n",
    "# course_link = driver.find_element(by=By.XPATH,value='//*[@id=\"1698390585510d\"]/div/div[1]/div/div/div/div[1]/div/div/div[2]/a[2]')\n",
    "# course_link.click()\n",
    "# time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467accc3",
   "metadata": {},
   "source": [
    "# What is XPATH\n",
    "\n",
    "Its a a language used to naviagte xml and html document in order to locate element within those document. In selenium it \n",
    "is used to find the element on webpage such as link, buttons ,text boxes so that they can be interacted with XPATH. \n",
    "Expression can be used to select based on their tag name , attribute or position within the document \n",
    "\n",
    "Handling scrolling || Saving Html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc909e",
   "metadata": {},
   "source": [
    "# SCRAPE DATA FROM AJIO WEBSITE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service('C:/Users/sasha/Desktop/chromedriver-win64/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service = s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45360137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doubt -- This code will get me the complete webpage after scrolling but could not able to get the html source code. \n",
    "# I do'nt Know why. \n",
    "\n",
    "driver.get(\"https://www.ajio.com/men-backpacks/c/830201001\")\n",
    "old_height = driver.execute_script('return document.body.scrollHeight')\n",
    "counter = 1\n",
    "while True:\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "    print(counter)\n",
    "    counter = counter+1\n",
    "    print(old_height)\n",
    "    print(new_height)\n",
    "    if new_height == old_height:\n",
    "        break\n",
    "    \n",
    "    old_height = new_height\n",
    "        \n",
    "html = driver.page_source\n",
    "with open('ajio.html','w',encoding = 'utf-8') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f587491",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ajio.html','r', encoding = 'utf-8') as f1:\n",
    "    ajio_html = f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ajio_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5623d",
   "metadata": {},
   "source": [
    "# WEBSCRAPING ON SMART PRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(\"C:/Users/sasha/Desktop/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.smartprix.com/mobiles\")\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ab22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(by=By.XPATH,value='//*[@id=\"app\"]/main/aside/div/div[5]/div[2]/label[1]/input').click()\n",
    "time.sleep(1)\n",
    "driver.find_element(by=By.XPATH,value='//*[@id=\"app\"]/main/aside/div/div[5]/div[2]/label[2]/input').click()\n",
    "\n",
    "old_height = driver.execute_script('return document.body.scrollHeight')\n",
    "while True:\n",
    "    driver.find_element(by=By.XPATH, value='//*[@id=\"app\"]/main/div[1]/div[3]/div[3]').click()\n",
    "    time.sleep(1)\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "    print(old_height)\n",
    "    print(new_height)\n",
    "    \n",
    "    if new_height == old_height:\n",
    "        break\n",
    "    old_height = new_height\n",
    "    \n",
    "html = driver.page_source\n",
    "\n",
    "with open('smartprix.html','w',encoding='utf-8') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd01bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('smartprix.html','r',encoding='utf-8') as f:\n",
    "    html1 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726667e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html1,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911448c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find_all('div',class_='sm-product has-tag has-features has-actions')\n",
    "container = soup.find_all('div',{'class':'sm-product has-tag has-features has-actions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('div',{'class':'sm-product has-tag has-features has-actions'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "price=[]\n",
    "spec_score=[]\n",
    "sim=[]\n",
    "processor=[]\n",
    "ram=[]\n",
    "battery=[]\n",
    "display=[]\n",
    "camera=[]\n",
    "memory=[]\n",
    "\n",
    "for i in container:\n",
    "    try:\n",
    "        name.append(i.find('h2').text.strip())\n",
    "    except:\n",
    "        name.append(np.nan)\n",
    "    try:\n",
    "        price.append(i.find('span',class_='price').text.strip())\n",
    "    except:\n",
    "        price.append(np.nan)\n",
    "    try:\n",
    "        spec_score.append(i.find('div',{'class':'score rank-1-bg'}).find('b').text)\n",
    "    except:\n",
    "        print(spec_score.append(np.nan))\n",
    "    try:\n",
    "        sim.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[0].text.strip())\n",
    "    except:\n",
    "        sim.append(np.nan)\n",
    "    try:\n",
    "        processor.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[1].text.strip())\n",
    "    except:\n",
    "        processor.append(np.nan)\n",
    "    try:\n",
    "        ram.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[2].text.strip())\n",
    "    except:\n",
    "        ram.append(np.nan)\n",
    "    try:\n",
    "        battery.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[3].text.strip())\n",
    "    except:\n",
    "        battery.append(np.nan)\n",
    "    try:\n",
    "        display.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[4].text.strip())\n",
    "    except:\n",
    "        display.append(np.nan)\n",
    "    try:\n",
    "        camera.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[5].text.strip())\n",
    "    except:\n",
    "        camera.append(np.nan)\n",
    "    try:\n",
    "        memory.append(i.find('ul',class_='sm-feat specs').find_all('li',class_='')[6].text.strip())\n",
    "    except:\n",
    "        memory.append(np.nan)\n",
    "    \n",
    "print(name)\n",
    "print(price)\n",
    "print(spec_score)\n",
    "print(sim)\n",
    "print(processor)\n",
    "print(ram)\n",
    "print(battery)\n",
    "print(display)\n",
    "print(camera)\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(name))\n",
    "print(len(price))\n",
    "print(len(spec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_data = pd.DataFrame({\n",
    "    \"model\":name,\n",
    "    \"Price\":price,\n",
    "    \"Rating\": spec_score,\n",
    "    \"Sim\":sim,\n",
    "    \"Processor\":processor,\n",
    "    \"Ram\":ram,\n",
    "    \"Battery\":battery,\n",
    "    \"Display\":display,\n",
    "    \"Camera\":camera,\n",
    "    \"Memory\":memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44356f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f9f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
